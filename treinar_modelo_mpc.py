# -*- coding: utf-8 -*-
"""Treinar modelo mpc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dol-VApMwp993knJWALfSvufNeBsfPfK
"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

from numpy import loadtxt
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.utils.vis_utils import plot_model
from keras import layers
from keras import initializers
from keras.callbacks import ModelCheckpoint
import tensorflow as tf
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import pandas_profiling #analisa todos os dados

import matplotlib.pyplot as plt
# %matplotlib inline
import string
import warnings
warnings.filterwarnings('ignore')
sampleRate = 100;
dados = pd.read_csv('dadosTreinamento1.csv');
# dados["indice"]=range( 0, 280000, 1);
dados["indice"]=range( 0, 280000, 1);
dados["tempo(s)"]=dados["indice"]*(1/sampleRate);

dados

max(dados["velocidade"])
min(dados["velocidade"])
np.std(dados["velocidade"])
max(dados["sinalDeControle"])
min(dados["sinalDeControle"])
np.std(dados["sinalDeControle"])

dadosTratados = pd.DataFrame();
dadosTratados["sinalDeControle"] = dados["sinalDeControle"];
dadosTratados["velocidade"] = dados["velocidade"];
dadosTratados["setpoint"] = dados["setpoint"];
dadosTratados

dadosTratados = pd.DataFrame();
#dadosTratados["velocidade"] = (dados["velocidade"]-50)/32.894318568744374;
dadosTratados["velocidade"] = dados["velocidade"];
dadosTratados["derivadaVelocidade"] = dadosTratados["velocidade"];
#dadosTratados["setpoint"] = (dados["setpoint"]-50)/32.894318568744374;
dadosTratados["setpoint"] = dados["setpoint"];
# dadosTratados["ierro"] = range( 0, 280000, 1);
dadosTratados["erro"] = range( 0, 280000, 1);
dadosTratados["ierro"] = range( 0, 280000, 1);
dadosTratados["derivadaErro"] = dadosTratados["velocidade"];
# dadosTratados["sinalDeControle"] = dados["sinalDeControle"]-14/23.39879335270195;
dadosTratados["sinalDeControle"] = dados["sinalDeControle"];

vanterior=0;
eanterior=0;
santerior=0;
ianterior=0;

for index, row in dadosTratados.iterrows():
  row["derivadaVelocidade"]=(row["velocidade"]-vanterior);
  vanterior = row["velocidade"];
  row["erro"] = row["velocidade"] - row["setpoint"];
  row["ierro"] = ianterior + row["erro"];
  if (row["ierro"]>255):
    row["ierro"] = 255
  elif (row["ierro"]<-255):
    row["ierro"] = -255
      
  row["derivadaErro"] = row["erro"] - eanterior;
  eanterior = row["erro"];
  ianterior=row["ierro"];

dadosTratados

# O rótulo utilizado para o processo de aprendizagem supervisionada será o sinal de controle e as entradas serão as referências impostas(setpoint) e os estados medidos do sistema(posição e velocidade = posição').
dadosArray = dadosTratados.to_numpy();
#X = dadosArray[201:,0:4]
X = dadosArray[201:,0]
y = dadosArray[201:,6]

X=np.c_[ X, dadosArray[201:, 2] ] 
X=np.c_[ X, dadosArray[201:, 3] ]

X[279798]

X_train = X[:223838]
X_test = X[223838:]
Y_train = y[:223838]
Y_test = y[223838:]

plt.plot(X[0:1000,0])

model = Sequential()
model.add(Dense(8, input_shape=(3,), activation='linear', kernel_initializer='random_normal', bias_initializer='zeros'))
#model.add(Dense(8, activation='tanh'))
model.add(Dense(4, activation='linear', kernel_initializer='random_normal', bias_initializer='zeros'))
model.add(Dense(1, activation='linear', kernel_initializer='random_normal', bias_initializer=keras.initializers.Constant(0)))

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

model.compile(loss='mse', optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001))

checkpoint = ModelCheckpoint("best_model.hdf5", monitor='loss', verbose=1,
    save_best_only=True, mode='auto', save_freq=6995*10)

history = model.fit(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test), batch_size=32,callbacks=[checkpoint])

# Get training and test loss histories
training_loss = history.history['loss']
test_loss = history.history['val_loss']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Perda no treinamento', 'Perda no teste'])
plt.xlabel('Épocas')
plt.ylabel('Perda')
plt.show();

tf.keras.models.save_model(model,'alou')

ypredict = model.predict(X);
#ypredict = ypredict.T

plt.plot(y[1000:1800])
plt.plot(np.round(ypredict[1000:1800]),'r')
plt.legend(['Sinal de controle MPC', 'Sinal de controle rede neural'])
plt.xlabel('Iterações')
plt.ylabel('Sinal de controle')
plt.show()

plt.plot(ypredict[1000:1800])

model.predict([[75,  15, 180]])